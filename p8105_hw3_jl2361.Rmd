---
title: "P8105 HW3"
author: "Jennifer Lee (UNI: jl2361)"
date: '`r format(Sys.time(), "%Y-%m-%d")`'
output: github_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(patchwork)
library(ggridges)

library(p8105.datasets)

knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%")

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis")

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

## Data description
We load the Instacart data from the p8105.datasets.
```{r}
data("instacart")

instacart =
  instacart %>%
  as_tibble(instacart)
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns with each row representing a single product from an instacart order. Variables include identifiers for user, order, and product; the order in which each product was added to the cart. There are also data describing the day and time of the order, and number of days since prior order. There are also several item-specific variables, describing the product name, department, and aisle, and whether the item has been ordered by this user in the past. In total, there are `r instacart %>% select(product_id) %>% distinct %>% count` products found in `r instacart %>% select(user_id, order_id) %>% distinct %>% count` orders from `r instacart %>% select(user_id) %>% distinct %>% count` distinct users.

## Tables & Plots
Below is a table summarizing the number of items ordered from aisle. In total, there are 134 aisles with fresh vegetables and fresh fruits holding the most items ordered.
```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Next is a plot that shows the number of items ordered in each aisle. Here, aisles are ordered by ascending number of items.
```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(aisle = fct_reorder(aisle, n)) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Our next table shows the three most popular items in aisles `baking ingredients`, `dog food care`, and `packaged vegetables fruits`, and includes the number of times each item is ordered.
```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(desc(n)) %>%
  knitr::kable()
```

Finally, here is a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week. This table has been formatted in an untidy manner for human readers. Pink Lady Apples are generally purchased slightly earlier in the day than Coffee Ice Cream, with the exception of day 5.

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_hour) %>%
  knitr::kable(digits = 2)
```

# Problem 2 

## Data description & cleaning
First, we load and tidy the data. The final dataset includes all originally observed variables and values, has cleaned variable names, includes a weekday vs weekend variable entitled `day_type`, converts `min` from a character to numeric variable, rounds `activity_counts` to whole numbers, and pivots the activity data from wide to long format. 
```{r}
accel_df = read_csv('data/accel_data.csv') %>% 
  janitor::clean_names() %>%
  mutate(
    day_type = if_else(day == "Saturday", "weekend", if_else(day == "Sunday", "weekend", "weekday")),
    day = fct_relevel(day, "Saturday", "Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday")) %>%
  select(week, day_id, day, day_type, everything()) %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "min", 
    names_prefix = "activity_",
    values_to = "activity_counts") %>%
  mutate(
    min = as.numeric(min),
    activity_counts = round(activity_counts, digits = 0)
  )
```

To briefly summarize, there are `r nrow(accel_df)` observations of activity counts for each minute of each day across the 5 week study period. Our final variables are: `week`, `day_id`, `day`, `day_type`, `min`, and `activity_counts`.

Next, we aggregate across minutes to create a total activity variable for each day entitled `total_daily_activity`, and create a table showing these totals. We notice that there seems to be a trend towards lower activity counts in the middle of the week (e.g. Tuesdays and Wednesdays), but there is substantial variability per week. For example: weeks 1-2, the highest activity counts seem to be recorded over the weekend vs. week 3, the highest activity count seems to be recorded on Monday vs. week 5, the highest activity counts seem to be recorded on Thursday through Friday. Of note, we notice a couple outliers for activity counts recorded on Saturday during weeks 4 and 5; 1440 seems to be a strange total activity count as it would indicate 1 count per minute over the course of the day. This needs further investigation.
```{r}
accel_df %>%
  group_by(week, day) %>%
  summarize(
    total_daily_activity = sum(activity_counts)) %>%
  pivot_wider(
    names_from = "day", 
    values_from = "total_daily_activity") %>%
  knitr::kable() 
```

## Plot
**Here is a single-panel plot that shows the 24-hour activity time courses for each day with color to indicate day of the week.** Based on this graph, it appears that activity counts seem to be generally lower during the middle of the week (e.g. Wednesdays) and higher on Fridays and weekend days. There seem to be spikes in activity counts around minutes 540, 720, 990, and 1260 when examining the activity data over the course of a single day. 
```{r}
accel_df %>%
  ggplot(
    aes(x = min, y = activity_counts, color = day)) +
      geom_line(alpha = .5) +
  labs(title = "Daily Activity Graph",
       x = "Minutes",
       y = "Activity Counts") +
  scale_x_continuous(
    breaks = c(0, 180, 360, 540, 720, 900, 1080, 1260, 1440)
  ) 
```

# Problem 3 

## Data description
We load the NY NOAA data from the p8105.datasets.
```{r}
data("ny_noaa") 

noaa_df = 
  ny_noaa %>%
  as_tibble(ny_noaa) 
```

To briefly summarize, there are `r nrow(noaa_df)` rows / observations and `r ncol(noaa_df)` columns / variables including: 

* `id`: weather station ID (character variable)
* `date`: date of observation (date variable) 
* `prcp`: precipitation in tenths of mm (integer variable) 
* `snow`: snowfall in mm (integer variable)  
* `snwd`: snow depth in mm (integer variable)
* `tmax`: maximum temperature in tenths of degrees C (character variable, which will need to be converted to a numerical variable)
* `tmin`: minimum temperature in tenths of degrees C (character variable, which will need to converted to a numerical variable)

Using the `summary` function, we notice that there are a lot of missing data in this dataset, particularly for the following variables: `prcp`, `snow`, `snwd`. We cannot meaningfully comment on `tmin` and `tmax` in the current state since they are assigned as character variables in the original dataset, but a glance at the data with the `head` function suggests that missing data is an issue with these variables too.
```{r}
summary(noaa_df)
head(noaa_df)
```

## Data cleaning
We clean variable names; create separate variables for year, month, and day and convert these to numeric variables; convert `tmax` and `tmin` to numeric variables. In terms of units, we divide the temperature values by 10 to convert units from tenths of degrees C to degrees C and divide the precipitation values by 10 to convert units from tenths of mm to mm. 
```{r}
noaa_df = 
  noaa_df %>%
  janitor::clean_names() %>%
  separate(col = date, into = c('year', 'month', 'day'), sep = "-") %>%
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
    year = as.numeric(year),
    month = as.numeric(month),
    day = as.numeric(day)) %>%
  mutate(
    tmax = tmax/10,
    tmin = tmin/10,
    prcp = prcp/10
  )
```

To find the most commonly observed value or mode for snowfall, we create a function to calculate the mode since R does not have a corresponding built-in function. We then create another dataset entitled `noaa_df_omit` where the missing data for snowfall are dropped, then find the mode, which seems to be 0 mm. From this result, we gather that mm may not be the most appropriate unit for snowfall (it seems too small!) and add a new variable entitled `snow_cm` to convert mm into cm.
```{r}
find_mode <- function(x) {
  u <- unique(x)
  tab <- tabulate(match(x, u))
  u[tab == max(tab)]
}

noaa_df_omit =
  noaa_df %>%
  drop_na(snow)

find_mode(pull(noaa_df_omit, snow))

noaa_df %>%
  mutate(
    snow_cm = snow / 10) %>%
  select(id, year, month, day, prcp, snow, snow_cm, everything())
```

## Plots
**Here is a two-panel plot showing the average max temperature in January and in July in each station across years.** As expected, the average tmax in January is generally lower compared to the average tmax in July. There appears to be a lot of year-to-year and station-to-station variability, which makes it challenging to make conclusions. 

In terms of outliers, 1994 and 2004 appear to have been unusually cold in January (i.e. lower average tmax); there was one station in 1988 that reported a much lower average tmax in July compared to other stations, and 1984, 2000, 2004, and 2007 appear to have been unseasonably cooler in July (i.e. lower average tmax).
```{r}
january_july_averagetmax = 
  noaa_df %>%
  filter(month == "1" | month == "7") %>%
  group_by(year, month, id) %>%
  summarize(
    average_tmax = mean(tmax, na.rm = TRUE)) %>% 
ggplot( 
  aes(x = year, y = average_tmax, group = id)) +
  geom_line(alpha = .2) +
  labs(
    title = "Average Maximum Temperature in January and July",
    y = "Temperature (degrees C)") +
  theme(legend.position = "none") +
  scale_x_continuous(
    breaks = c(1980, 1982, 1984, 1986, 1988, 1990, 1992, 1994, 1996, 1998, 2000, 2002, 2004, 2006, 2008, 2010)) + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) + 
  facet_grid(. ~ month, labeller = label_both) 

january_july_averagetmax
```

**Here is a two-panel plot showing (i) tmax vs tmin for the full dataset using a hexagonal heatmap; and (ii) the distribution of snowfall values greater than 0 and less than 100 mm separately by year using a ridge plot.**
```{r}
tmax_tmin = 
  noaa_df %>%
  ggplot(
    aes(x = tmax, y = tmin)) + 
  geom_hex() + 
  labs(x = "T Max (degrees C)",
       y = "T Min (degrees C)") +
  theme(legend.text = element_text(size = 5))
 
snowfall = 
  noaa_df %>%
  filter(snow > 0, snow < 100) %>%
  mutate(
    year = as.character(year)) %>%
  ggplot(
    aes(x = snow, y = year)) +
       geom_density_ridges(scale = .5) +
  labs(x = "Snowfall (mm)",
       y = "Year")

tmax_tmin + snowfall
```